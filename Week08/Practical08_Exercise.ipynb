{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL - DQN with Target Network\n",
    "\n",
    "### Import required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abhch\\Documents\\Repos\\ECE4078\\Pracs\\ECE4078_Practicals_2023\\Week08\\Practical08_Exercise.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPractical08_Support\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrender\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhch\\Documents\\Repos\\ECE4078\\Pracs\\ECE4078_Practicals_2023\\Week08\\Practical08_Support\\render.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m animation\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from Practical08_Support.render import *\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sample_agent = 'Practical08_Support/DQNagent_sample.pt'\n",
    "\n",
    "def set_seed(env, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.reset(seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DQN Algorithm with Target Network\n",
    "\n",
    "In this notebook we will extend DQN by adding a target network. This revised version of the DQN algorithm is shown below\n",
    "\n",
    "![DQN_algorithm.png](https://i.postimg.cc/15tMBhjG/DQN-algorithm.png)\n",
    "\n",
    "### The main changes are:\n",
    "\n",
    "- We have extended the Agent class' attributes to include 2 DQN networks instead of one (one target and one policy network)\n",
    "\n",
    "- We have change the method ``get_next_q(.)`` so the q-values are computed using the target network instead of the policy network\n",
    "\n",
    "- We have added a new method called ``transfer_parameters``(.). This method replaces the parameters of the target network with those of the policy network \n",
    "\n",
    "- We have modified the main loop to include instructions that call ``transfer_parameters``(.) after a predefined number of episodes\n",
    "\n",
    "Compared to DQN, DQN with a target network is more stable and robust, since it updates the network parameters at a lower frequency. It helps with the \"catastrophic forgetting\" problem that you may have observed in DQN.\n",
    "\n",
    "## Replay Buffer\n",
    "\n",
    "We use the same Replay Buffer implementation\n",
    "\n",
    "**Note**: This implementation of the ReplayMemory class was taken from [***Pytorch DQN tutorial***](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tuple represents one observation in our environment\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    A cyclic buffer of bounded size (capacity) that holds the transitions \n",
    "    observed recently. \n",
    "    \n",
    "    It also implements a sample() method for selecting a random \n",
    "    batch of transitions for training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Returns a minibatch of `Transition` randomly\n",
    "        Args:\n",
    "            batch_size (int): Size of mini-bach\n",
    "        Returns:\n",
    "            List[Transition]: Minibatch of `Transition`\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Network\n",
    "\n",
    "Let us now define the Multi Layer Perceptron network that will be used as the function approximator for the action-value function (q-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"DQN Network\n",
    "        Args:\n",
    "        input_dim (int): `state` dimension.\n",
    "        output_dim (int): Number of actions.\n",
    "        hidden_dim (int): Hidden dimension in fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs=8, num_actions=4, hidden_dim_1=32, hidden_dim_2=32):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_2, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns a Q_value\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, num_inputs)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 2-D tensor of shape (n, num_actions)\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent\n",
    "\n",
    "This class contains the main steps of the Deep Q-learnig algorithm.\n",
    "\n",
    "Please complete the impletation by filling in two missing parts:\n",
    "- TODO 1: Add a target network (under the ```__init__``` function)\n",
    "- TODO 2: Compute the loss (under the ```optimize``` function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    \"\"\"DQN Agent\n",
    "    This class contains the main steps of the DQN algorithm\n",
    "    \n",
    "    Attributes:\n",
    "    action_value_net (DQN): Function approximator for our action-value function (predictor)\n",
    "    target_net (DQN): Function approximator for our target action-value function\n",
    "    loss_fn (MSELoss): Criterion that measures the mean squared error (squared L2 norm) \n",
    "                       between each element of the predicted and target q-values.\n",
    "    optimizer (Adam): Stochastic gradient optimize\n",
    "    gamma (float): Discount factor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=8, output_dim=4, \n",
    "                 hidden_dim_1=32, hidden_dim_2=32, gamma=0.99, lr=0.0001):\n",
    "        \"\"\"\n",
    "        Define instance of DQNAgent\n",
    "        Args:\n",
    "        input_dim (int): `state` dimension.\n",
    "        output_dim (int): Number of actions.\n",
    "        hidden_dim (int): Hidden dimension in fully connected layer\n",
    "        \"\"\"\n",
    "        self.action_value_net = DQN(input_dim, output_dim, hidden_dim_1, hidden_dim_2).to(device)\n",
    "                \n",
    "        #TODO 1: Add a target network. Make sure both networks start with same parameters ----------\n",
    "        self.target_net = DQN(input_dim, output_dim, hidden_dim_1, hidden_dim_2).to(device)\n",
    "        #ENDTODO -----------------------------------------------------------------------------------\n",
    "                        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.action_value_net.parameters(), lr=lr)\n",
    "                \n",
    "        self.gamma = torch.tensor(gamma).float().to(device)\n",
    "        \n",
    "    def obtain_action(self, state, action_space_dim, epsilon):\n",
    "        \"\"\"\n",
    "        Select next action using epsilon-greedy policy\n",
    "        Args:\n",
    "        epsilon (float): Threshold used to decide whether a random or maximum-value action \n",
    "                         should be taken next\n",
    "         Returns:\n",
    "            int: action index\n",
    "        \"\"\"        \n",
    "        with torch.no_grad():\n",
    "            cur_q = self.action_value_net(torch.from_numpy(state).float().to(device))\n",
    "        q_value, action = torch.max(cur_q, axis=0)\n",
    "        action = action if torch.rand(1,).item() > epsilon else torch.randint(0, action_space_dim, (1,)).item()\n",
    "        action = torch.tensor([action]).to(device)\n",
    "        return action\n",
    "    \n",
    "    def get_next_q(self, state):\n",
    "        \"\"\"Returns Q_value for maximum valued action at each state s\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, num_inputs)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 1 tensor of shape (n)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(state)\n",
    "        q, _ = torch.max(next_q, axis=1)\n",
    "        return q\n",
    "    \n",
    "    def optimize(self, batch):\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        Args:\n",
    "            batch: List[Transition]: Minibatch of `Transition`\n",
    "        Returns:\n",
    "            float: loss value\n",
    "        \"\"\"\n",
    "        \n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "                \n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state is the one after which the simulation ends)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s.item() is not True,\n",
    "                                          batch.done)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.stack([s for i, s in enumerate(batch.next_state)\n",
    "                                            if batch.done[i].item() is not True])\n",
    "\n",
    "        # Compute predicted q-values\n",
    "        predicted_q = self.action_value_net(state_batch).gather(1, action_batch).reshape(1,-1)\n",
    "        \n",
    "        # Compute expected values for non-terminal and terminal states (this is our TD target)\n",
    "        target_q = torch.zeros(len(batch.state), device=device)\n",
    "        target_q[non_final_mask] = self.get_next_q(non_final_next_states)\n",
    "        expected_q = reward_batch.reshape(1,-1)+(self.gamma * target_q)\n",
    "        \n",
    "        #TODO 2: Compute loss ----------------------------------------------------------------------\n",
    "        loss = self.loss_fn(expected_q, predicted_q)\n",
    "        #ENDTODO -----------------------------------------------------------------------------------\n",
    "        \n",
    "        # Use loss to compute gradient and update policy parameters through backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "                \n",
    "        return loss.item()\n",
    "    \n",
    "    def transfer_parameters(self):\n",
    "        \"\"\"Transfer parameters from action-value to target network\n",
    "        \"\"\"\n",
    "        self.target_net.load_state_dict(self.action_value_net.state_dict())\n",
    "        self.target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Parameters\n",
    "\n",
    "The parameter ``freq_sync`` defines how often parameters are transferred between networks. Here for every 10 episodes we copy the parameters over to the target network.\n",
    "\n",
    "Note that here we are controlling the training duration by the number of episodes, instead of using the number of frames as in DQN. This is why the epsilon_decay is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Epsilon')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define running hyper-parameters and epsilon training sequence\n",
    "# Feel free to change the parameters\n",
    "memory_capacity = 2500\n",
    "batch_size = 64\n",
    "num_episodes = 100\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 10\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "hidden_dim_1 = 32\n",
    "hidden_dim_2 = 32\n",
    "freq_sync = 10\n",
    "seed_value = 42\n",
    "\n",
    "epsilon_by_step = lambda ep_idx: epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * ep_idx / epsilon_decay)\n",
    "\n",
    "# Plotting out epsilon over episodes that should match what you expect\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot([epsilon_by_step(i) for i in range(num_episodes)])\n",
    "ax.set_xlabel(\"Num. episodes\")\n",
    "ax.set_ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop and Replay Buffer Control\n",
    "\n",
    "This is the main loop of our DQN implementation. Here we generate the samples added to the replay memory and train the agent using a batch sampled for the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "box2D is not installed, run `pip install gym[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhch\\.conda\\envs\\ece4078\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py:14\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         circleShape,\n\u001b[0;32m     17\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         revoluteJointDef,\n\u001b[0;32m     22\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abhch\\Documents\\Repos\\ECE4078\\Pracs\\ECE4078_Practicals_2023\\Week08\\Practical08_Exercise.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m list_epsilon \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m replay_buffer \u001b[39m=\u001b[39m ReplayMemory(memory_capacity)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mLunarLander-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m set_seed(env, seed_value)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhch/Documents/Repos/ECE4078/Pracs/ECE4078_Practicals_2023/Week08/Practical08_Exercise.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m n_actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n",
      "File \u001b[1;32mc:\\Users\\abhch\\.conda\\envs\\ece4078\\lib\\site-packages\\gym\\envs\\registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m     env_creator \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mentry_point\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     \u001b[39m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     env_creator \u001b[39m=\u001b[39m load(spec_\u001b[39m.\u001b[39;49mentry_point)\n\u001b[0;32m    583\u001b[0m mode \u001b[39m=\u001b[39m _kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    584\u001b[0m apply_human_rendering \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhch\\.conda\\envs\\ece4078\\lib\\site-packages\\gym\\envs\\registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m    Calls the environment constructor\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[0;32m     62\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\Users\\abhch\\.conda\\envs\\ece4078\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:972\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\abhch\\.conda\\envs\\ece4078\\lib\\site-packages\\gym\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbipedal_walker\u001b[39;00m \u001b[39mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcar_racing\u001b[39;00m \u001b[39mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlunar_lander\u001b[39;00m \u001b[39mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32mc:\\Users\\abhch\\.conda\\envs\\ece4078\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py:24\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         circleShape,\n\u001b[0;32m     17\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         revoluteJointDef,\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mraise\u001b[39;00m DependencyNotInstalled(\u001b[39m\"\u001b[39m\u001b[39mbox2D is not installed, run `pip install gym[box2d]`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     28\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
     ]
    }
   ],
   "source": [
    "losses_list, rewards_list, episode_len_list = [], [], []\n",
    "list_epsilon = []\n",
    "replay_buffer = ReplayMemory(memory_capacity)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "set_seed(env, seed_value)\n",
    "n_actions = env.action_space.n\n",
    "dim_state = env.observation_space.shape[0]\n",
    "\n",
    "agent = DQNAgent(input_dim=dim_state, \n",
    "                 output_dim=n_actions, \n",
    "                 hidden_dim_1=hidden_dim_1, \n",
    "                 hidden_dim_2=hidden_dim_2, \n",
    "                 gamma=gamma, lr=lr)\n",
    "\n",
    "cur_epsilon = epsilon_start\n",
    "\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "\n",
    "    state, is_finished, ep_len, losses, rewards = env.reset()[0], False, 0, 0, 0\n",
    "\n",
    "    cur_epsilon = epsilon_by_step(i_episode+1)\n",
    "    list_epsilon += [cur_epsilon]\n",
    "    while not is_finished:\n",
    "        ep_len += 1\n",
    "        action = agent.obtain_action(state, n_actions, cur_epsilon)\n",
    "        next_state, reward, done, truncated, _ = env.step(action.item())\n",
    "        is_finished = done or truncated\n",
    "        rewards += reward\n",
    "\n",
    "        t_s = torch.tensor(state).float().to(device)\n",
    "        t_r = torch.tensor([reward]).float().to(device)\n",
    "        t_ns = torch.tensor(next_state).float().to(device)\n",
    "        t_a = action.to(device)\n",
    "        t_done = torch.tensor([is_finished]).bool().to(device)\n",
    "\n",
    "        replay_buffer.push(t_s, t_a, t_ns, t_r, t_done)\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            transitions = replay_buffer.sample(batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            loss = agent.optimize(batch)\n",
    "            losses += loss\n",
    "\n",
    "    losses_list.append(losses / ep_len)\n",
    "    rewards_list.append(rewards)\n",
    "    episode_len_list.append(ep_len)\n",
    "\n",
    "    # Add rule that call transfer_parameters() every freq_sync episodes\n",
    "    if i_episode % freq_sync == 0:\n",
    "        agent.transfer_parameters()\n",
    "\n",
    "    # Every 10 episodes we plot the approximator's progress and performance\n",
    "    if i_episode % 10 == 0:\n",
    "        plot_ep(i_episode, rewards_list, losses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\abhch\\Documents\\Repos\\ECE4078\\Pracs\\ECE4078_Practicals_2023\\.conda\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\abhch\\Documents\\Repos\\ECE4078\\Pracs\\ECE4078_Practicals_2023\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# save the trained network, this is what you will submit along with your ipynb\n",
    "file_name = 'Practical08_Exercise.pt' # replace 12345 with your student ID\n",
    "agent.seed = seed_value\n",
    "torch.save(agent, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "You will be graded based on the performance of your network. Your solution will be executed for a total of 100 trials and the average return will be used to determine your grade. The grading scale is:\n",
    "\n",
    "| Avg. Return | Marks       |\n",
    "| ----------- | ----------- |\n",
    "| < 0         | 1  pt       |\n",
    "| [0,50)      | 2  pts      |\n",
    "| [50, 100)   | 3  pts      |\n",
    "| [100, 150)  | 4  pts      |\n",
    "| [150, 200)  | 5  pts      |\n",
    "| ≥ 200       | 6  pts      |\n",
    "\n",
    "Here is the exact grading procedure: \n",
    "- We will grade the performance of the `.pt` file, so we will run `Practical08_Exercise_Grading.ipynb` notebook in the grader (not the `.ipynb` you submitted), obtain `mean_return`, and just compare its value against the table above, feel free to run `Practical08_Exercise_Grading.ipynb` to test your `.pt` file.\n",
    "- We will NOT run your `.ipynb`, instead, we will inspect it manually to see if there is some malicious behaviour!\n",
    "\n",
    "# For your submission:\n",
    "- Submit both a copy of THIS notebook and the trained network following the naming convention (*StudentID_Practical08.ipynb* and *Practical08_Exercise.pt*)\n",
    "- You can tune the hyper-parameters if needed or modify the architecture of the DQN approximator (don't change the name of the class).\n",
    "- If you make changes to the architecture if the `DQN` class, please only make changes inside the `nn.Sequential` (if you wish to) because that will get saved in the `.pt` file properly. If you add more things elsewhere in the `DQN` class, there is no guarantee that the grading notebook (`Practical08_Exercise_Grading.ipynb`) will import `.pt` properly and your network might perform differently than what you expected. \n",
    "- Also note that different machines and setup might have different randomness in it too, there is no guarantee that the result you get is what you will get on my end, the only way to make sure that I will get the same number is using Deepnote or Binder for cloud and Docker for local install to check your `.pt` file. Meaning you can train the agent in whatever platform you wish to, but you should check your trained `.pt` in Deepnote or Binder for cloud and Docker for local install"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
